{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/inteligenciafrvm/inteligenciafrvm/blob/master/Clases%20pr%C3%A1cticas/11.%20Intro%20al%20aprendizaje%20por%20refuerzos%20profundo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SytTR8K1oII-"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BFrF74foIJD"
   },
   "source": [
    "Créditos:\n",
    "\n",
    "* Documentación y repo de Stable-baselines https://stable-baselines3.readthedocs.io.\n",
    "    * Tutorial sobre SB3: https://github.com/araffin/rl-tutorial-jnrr19.\n",
    "* Documentación y repo de OpenAI Gym https://github.com/openai/gym/blob/master/docs/.\n",
    "    * Crear un entorno https://github.com/openai/gym/blob/master/docs/creating-environments.md. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlZ0ehGZoIJD"
   },
   "source": [
    "Stable-baselines3: framework de deep RL que provee interfaces para ejecutar y adaptar algoritmos de RL \"al estilo scikit-learn\". Permite utilizar agentes abstrayéndonos de los detalles de bajo nivel de abstracción referentes a la implementación del algoritmo$^1$\n",
    "\n",
    "Además, ofrece herramientas muy útiles como\n",
    "\n",
    "* Monitores que permiten ver el rendimiento del agente según se desempeña en el entorno, sin tener que esperar a que finalice de entrenar.\n",
    "* Callbacks que permiten accionar eventos cuando se cumplen algunas condiciones en el entrenamiento de nuestro agente (por ejemplo, detenerlo si la recompensa recibida es menor a cierto umbral tras un cierto período de tiempo).\n",
    "\n",
    "\n",
    "Documentación https://stable-baselines3.readthedocs.io\n",
    "\n",
    "Es un fork activamente mantenido de [OpenAI baselines](https://github.com/openai/baselines)\n",
    "\n",
    "La versión 3 cambia el framework subyacente de Tensorflow a Pytorch y está activamente en desarrollo; no obstante la versión 2 es completamente funcional\n",
    "\n",
    "$^1$ no obstante, al igual que sucede generalmente con librerías de ML: \n",
    "\n",
    "* Siempre es bueno tener en mente las características, ventajas y desventajas del algoritmo utilizado, pues de eso depende mucho la convergencia de nuestra solución, especialmente cuando se emplean entornos adaptados para nuestras necesidades. \n",
    "\n",
    "* Esta librería, al igual que demás frameworks generales de RL, están muy probadas en entornos estándares de RL como Atari o PyBullet. No obstante, es posible que nuestro entorno o nuestras necesidades difieran significativamente, lo que hace que en algunos casos haya que meter mano directo en el código de los algoritmos/librería."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4SWX4wuoIJE"
   },
   "source": [
    "# Interfaz básica stable-baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SD-FSSuKoIJF"
   },
   "source": [
    "### Instalación de Stable-baselines\n",
    "\n",
    "Desde Linux o Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ttqOLol_oIJG",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e7b26d16-a6d3-45af-ca5f-332a2330427a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/DLR-RM/stable-baselines3\n",
      "  Cloning https://github.com/DLR-RM/stable-baselines3 to /tmp/pip-req-build-l6ggwal5\n",
      "  Running command git clone -q https://github.com/DLR-RM/stable-baselines3 /tmp/pip-req-build-l6ggwal5\n",
      "Requirement already satisfied (use --upgrade to upgrade): stable-baselines3==1.1.0a11 from git+https://github.com/DLR-RM/stable-baselines3 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages\n",
      "Requirement already satisfied: gym>=0.17 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3==1.1.0a11) (0.17.3)\n",
      "Requirement already satisfied: numpy in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3==1.1.0a11) (1.20.2)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3==1.1.0a11) (1.8.1)\n",
      "Requirement already satisfied: cloudpickle in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3==1.1.0a11) (1.2.2)\n",
      "Requirement already satisfied: pandas in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3==1.1.0a11) (1.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from stable-baselines3==1.1.0a11) (3.4.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from gym>=0.17->stable-baselines3==1.1.0a11) (1.5.0)\n",
      "Requirement already satisfied: scipy in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from gym>=0.17->stable-baselines3==1.1.0a11) (1.6.2)\n",
      "Requirement already satisfied: typing-extensions in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from torch>=1.4.0->stable-baselines3==1.1.0a11) (3.7.4.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from pandas->stable-baselines3==1.1.0a11) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from pandas->stable-baselines3==1.1.0a11) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from matplotlib->stable-baselines3==1.1.0a11) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from matplotlib->stable-baselines3==1.1.0a11) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from matplotlib->stable-baselines3==1.1.0a11) (2.4.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from matplotlib->stable-baselines3==1.1.0a11) (7.1.2)\n",
      "Requirement already satisfied: future in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3==1.1.0a11) (0.17.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->stable-baselines3==1.1.0a11) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/juan/anaconda3/envs/python37/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->stable-baselines3==1.1.0a11) (46.4.0.post20200518)\n",
      "Building wheels for collected packages: stable-baselines3\n",
      "  Building wheel for stable-baselines3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stable-baselines3: filename=stable_baselines3-1.1.0a11-py3-none-any.whl size=160797 sha256=9b2f8f6d24f474c13543defededfbc7a95d64acb5fe48e7e5d5e3d3e1caf041f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4ohuirha/wheels/2b/88/65/5d0cb266b061107af8c518096240bea8578e9843716f79e4da\n",
      "Successfully built stable-baselines3\n"
     ]
    }
   ],
   "source": [
    "#@title Instalación (no modificar)\n",
    "!pip install stable-baselines3[extra,tests,docs]>=0.11.0a4\n",
    "!pip install git+https://github.com/DLR-RM/stable-baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCtE5l9qr7sL"
   },
   "source": [
    "Desde Windows, además, instalar: \n",
    "* Microsoft Visual C++ desde https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "* PyType, mediante `conda install -c conda-forge pytype`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUUkScRxoIJQ"
   },
   "source": [
    "### Instalación de RLBaselinesZoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiKPJUfdoIJQ"
   },
   "source": [
    "Desde Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZM_pM0mIoIJQ"
   },
   "outputs": [],
   "source": [
    "#@title Instalación de RLBaselinesZoo (no modificar)\n",
    "\n",
    "IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "    !cd rl-baselines3-zoo/\n",
    "    !apt-get install swig cmake ffmpeg\n",
    "    !pip install -r /content/rl-baselines3-zoo/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzBEoyPzoIJR"
   },
   "source": [
    "Desde Linux, ejecutando\n",
    "\n",
    "    git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "    cd rl-baselines3-zoo/\n",
    "    sudo apt-get install swig cmake ffmpeg\n",
    "    pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBBnh36Tr7sM"
   },
   "source": [
    "### Instalación desde servicios como Google Cloud\n",
    "\n",
    "    wget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh\n",
    "    chmod 755 Anaconda3-2020.11-Linux-x86_64.sh\n",
    "    ./Anaconda3-2020.11-Linux-x86_64.sh\n",
    "    conda create --name rl\n",
    "    conda activate rl\n",
    "    conda config --add channels conda-forge\n",
    "    conda install jupyter atari_py swig\n",
    "    pip install stable-baselines3[extra,tests,docs]>=0.11.0a4\n",
    "    pip install sb3-contrib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUpOonyoIJH"
   },
   "source": [
    "## Ejecución de un algoritmo de RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Morjd1dFoIJH"
   },
   "source": [
    "### Importaciones/inicializaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OG7i44kqoIJH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "#from gym.envs.registration import register\n",
    "\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is2ytf-coIJH"
   },
   "source": [
    "### Ejemplo básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8SqFh7noIJI",
    "outputId": "1d56db34-53bd-450a-8fe8-825f7201ffcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f29eef38a50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# MlpPolicy es una política \"estándar\" que aprende con perceptron multicapa\n",
    "# (es decir sin capas convolucionales o demás variantes),\n",
    "# 2 capas ocultas con 64 neuronas cada una\n",
    "model = DQN('MlpPolicy', env)\n",
    "model.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f5JFC7AoIJK"
   },
   "source": [
    "### Renderización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5nIL0TuwoIJK"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "\n",
    "    obs = env.reset()\n",
    "    for i in range(1000):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "          obs = env.reset()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTtXPFd2oIJK"
   },
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFYyMrvboIJK"
   },
   "source": [
    "#### Ver rendimiento del agente en tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pC1hqla2oIJK",
    "outputId": "77c1b83c-9952-4ee7-b4eb-2c8bfa0d2024"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f29e14b4790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venv = make_vec_env(lambda: gym.make('CartPole-v1'), n_envs=1)\n",
    "\n",
    "model = DQN('MlpPolicy', venv, tensorboard_log='tensorboard/')\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rldIL1u4oIJL"
   },
   "source": [
    "Para verlo en tensorboard, correr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4Db-mCxoIJL"
   },
   "source": [
    "`tensorboard --logdir=tensorboard/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "2AfqEmCWxWFv",
    "outputId": "e2297f30-400f-4480-b04e-90f1b4dc2549"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 20152), started 4:07:20 ago. (Use '!kill 20152' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-296fc1cf21f54ad8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-296fc1cf21f54ad8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u0_1dk5oIJL"
   },
   "source": [
    "### Monitor\n",
    "\n",
    "Vamos a crear un monitor (es uno de los *wrappers* que nos provee SB3) para loguear nuestro agente en la carpeta logs. Nuestro monitor guardará datos de recompensa (r), duración (l) y tiempo total (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7uF7sQdoIJL",
    "outputId": "ea0fb222-58aa-4c18-9c8c-ae97132ee3ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f2a68b8c950>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env = Monitor(env, 'logs/')  # reemplazamos env por su monitor\n",
    "\n",
    "model = DQN('MlpPolicy', env, )\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yVVSGjvoIJM"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son funciones que se llaman al cumplirse determinadas condiciones en la ejecución, y permiten realizar acciones o evaluar el estado del agente que se está ejecutando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47XdjvaSoIJM",
    "outputId": "8eb9dd8d-ca66-4aee-cdbf-bce8a510ecea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/envs/python37/lib/python3.7/site-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=9.60 +/- 0.80\n",
      "Episode length: 9.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=9.20 +/- 0.75\n",
      "Episode length: 9.20 +/- 0.75\n",
      "Eval num_timesteps=3000, episode_reward=9.80 +/- 0.75\n",
      "Episode length: 9.80 +/- 0.75\n",
      "New best mean reward!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f2a68b9d710>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "callbacks = []  # lista de callbacks a usar, pueden ser varios\n",
    "\n",
    "# callback para detener entrenamiento al alcanzar recompensa de 9.8\n",
    "# (a fines demostrativos, es una recompensa baja)\n",
    "stop_training_callback = StopTrainingOnRewardThreshold(reward_threshold=9.8)\n",
    "\n",
    "# al crear EvalCallback, se asocia el mismo con stop_training_callback\n",
    "callbacks.append(EvalCallback(env, \n",
    "                              eval_freq=1000,\n",
    "                              callback_on_new_best=stop_training_callback))\n",
    "\n",
    "# la semilla aleatoria hace que las ejecuciones sean determinísticas\n",
    "model = DQN('MlpPolicy', env, seed=42)\n",
    "model.learn(total_timesteps=10000, callback=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCPD7oaNoIJM"
   },
   "source": [
    "### Ejecutar agente RL en múltiples ambientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNiTT2AzoIJM"
   },
   "source": [
    "Esta librería provee una interfaz para ejecutar agentes en varias instancias de un mismo entorno a la vez (*vectorized environments*), de modo tal que se habilite la ejecución paralela y de otras funcionalidades útiles.\n",
    "\n",
    "Para ello, varios de sus algoritmos implementan cambios que consideren la posibilidad de que haya múltiples entornos subyacentes, por ejemplo `step(accion)` cambia a `step(lista_acciones)`, aplicando acciones a todos los entornos, recibiendo ahora múltiples observaciones y recompensas.\n",
    "\n",
    "Otro cambio: se aplica `reset()` automáticamente a cada entorno que llega a un estado final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GGZjjcDoIJN"
   },
   "source": [
    "SB brinda dos formas de utilizar entornos vectorizados:\n",
    "\n",
    "* **DummyVecEnv**, el cuál consiste en un *wrapper* de varios entornos, los cuáles funcionarán en un sólo hilo. Este wrapper es útil como entrada de algoritmos que requieren los entornos de esta forma, y habilita los procesamientos y operaciones comunes de los entornos vectorizados.\n",
    "* **SubprocVecEnv**, el cuál paraleliza multiples entornos pero en procesos de ejecucíon separados. Cada proceso tiene su propia memoria y puede adquirir derechos sobre las CPUs de la computadora donde se ejecuta. Se utiliza cuando el entorno del agente es computacionalemente complejo. Atención! **Puede comer mucha RAM**.\n",
    "\n",
    "Vemos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovc6JDmYoIJN",
    "outputId": "6f36d2d0-bc4e-4299-b612-58103bc73f6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f2a68b43450>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplo de ambiente dummy\n",
    "venv = DummyVecEnv([lambda: gym.make('CartPole-v1')]*4)\n",
    "\n",
    "model = PPO('MlpPolicy', venv, )\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkadM0S1oIJO"
   },
   "source": [
    "También puede hacerse con un una función de SB a tal efecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wez-ZY_IoIJO",
    "outputId": "bc30a171-8081-4b97-ba76-063551b2a200"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f2a68adf6d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venv = make_vec_env(lambda: env, n_envs=4)\n",
    "\n",
    "model = PPO('MlpPolicy', venv, )\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjH4ZQ4NoIJO"
   },
   "source": [
    "### Ejecutar agente con políticas personalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jp_aPhIdoIJO",
    "outputId": "19b3f2a7-2821-4fe9-a749-b32be258aee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.1     |\n",
      "|    ep_rew_mean     | 23.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 1221     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.4        |\n",
      "|    ep_rew_mean          | 25.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009700403 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | -0.00103    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.77        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 42.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 31.1        |\n",
      "|    ep_rew_mean          | 31.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 786         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008749355 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 30.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 42.3       |\n",
      "|    ep_rew_mean          | 42.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 745        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01923478 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.625     |\n",
      "|    explained_variance   | 0.348      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 19.3       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    value_loss           | 41         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 58.7        |\n",
      "|    ep_rew_mean          | 58.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 734         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045803316 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.611      |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.00332     |\n",
      "|    value_loss           | 56.6        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f2a68acfb10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos una clase con una red neuronal de 128x128 neuronas\n",
    "\n",
    "model = PPO('MlpPolicy', policy_kwargs=dict(net_arch=[128,128]), env='CartPole-v1', verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbwkBi87oIJO"
   },
   "source": [
    "### Utilizar un entorno personalizado\n",
    "\n",
    "Antes que nada, además de la interfaz que ya vimos de Gym, hay otras nociones que tenemos que tener en cuenta en este contexto:\n",
    "\n",
    "* Los entornos definen un espacio de estados y de acciones, a partir de los cuáles los modelos asumen y respetan la \"forma\" de observaciones y acciones. Por ejemplo, algunos algoritmos están diseñados para espacios de acciones discretos (DQN), continuos (DDPG) o bien poseen implementaciones particulares pueden usarse en ambos (PPO, en el repo de SB3). En cuanto a los espacios, algunos algoritmos asumen explícitamente un espacio discreto (y pequeño), como Q-Learning, mientras que otros como PPO asumen cualquier tipo de espacio.\n",
    "* Los dos tipos más comunes de estados o acciones son los espacios discretos `gym.spaces.Discrete` y los continuos `gym.spaces.Box`.\n",
    "* Los espacios discretos definen un conjunto de $n$ estados/acciones $\\{ 0, 1, \\dots, n-1 \\}$, mientras que los espacios continuos definen un espacio $\\mathbb{R}^d$, de una de las siguientes 4 formas: $[a, b], (-\\infty, b], [a, \\infty), (-\\infty, \\infty)$, en donde $a,b$ son las cotas superior e inferior (de existir).\n",
    "* Ejemplos: un espacio de acciones `Discrete(4)` tiene 4 acciones: $\\{0,1,2,3\\}$; un espacio de estados `Discrete(16)` tiene 16 estados. Un espacio de estados ALTURA, ANCHO, N_CANALES que represente una imagen RGB acotada en $[a=0, b=255]$ se puede crear como\n",
    "\n",
    "`observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFLZieLNoIJO"
   },
   "source": [
    "Para usar un entorno compatible por esta librería, el mismo tiene que heredar de *gym.Env*. Vemos un ejemplo (crédito: https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "str0v6DUoIJO"
   },
   "outputs": [],
   "source": [
    "class GoLeftEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Ambiente personalizado que sigue la interfaz de gym.\n",
    "  Es un entorno simple en el cuál el agente debe aprender a ir siempre \n",
    "  hacia la izquierda.\n",
    "  \"\"\"\n",
    "  # Dado que estamos en colab, no podemos implementar la salida por interfaz \n",
    "  # gráfica ('human' render mode) \n",
    "  metadata = {'render.modes': ['console']}\n",
    "  # Definimos las constantes\n",
    "  LEFT = 0\n",
    "  RIGHT = 1\n",
    "\n",
    "  def __init__(self, grid_size=10):\n",
    "    super(GoLeftEnv, self).__init__()\n",
    "\n",
    "    # Tamaño de la grilla de 1D\n",
    "    self.grid_size = grid_size\n",
    "    # Inicializamos en agente a la derecha de la grilla\n",
    "    self.agent_pos = grid_size - 1\n",
    "\n",
    "    # Definimos el espacio de acción y observaciones\n",
    "    # Los mismos deben ser objetos gym.spaces\n",
    "    # En este ejemplo usamos dos acciones discretas: izquierda y derecha\n",
    "    n_actions = 2\n",
    "    self.action_space = spaces.Discrete(n_actions)\n",
    "    # La observación será la coordenada donde se encuentra el agente\n",
    "    # puede ser descrita tanto por los espacios Discrete como Box\n",
    "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
    "                                        shape=(1,), dtype=np.float32)\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Importante: la observación devuelta debe ser un array de numpy\n",
    "    :return: (np.array) \n",
    "    \"\"\"\n",
    "    # Se inicializa el agente a la derecha de la grilla\n",
    "    self.agent_pos = self.grid_size - 1\n",
    "    # convertimos con astype a float32 (numpy) para hacer más general el agente\n",
    "    # (en caso de que querramos usar acciones continuas)\n",
    "    return np.array([self.agent_pos]).astype(np.float32)\n",
    "\n",
    "  def step(self, action):\n",
    "    if action == self.LEFT:\n",
    "      self.agent_pos -= 1\n",
    "    elif action == self.RIGHT:\n",
    "      self.agent_pos += 1\n",
    "    else:\n",
    "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
    "\n",
    "    # Evitamos que el agente se salga de los límites de la grilla\n",
    "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "    # Llegó el agente a su estado objetivo (izquierda) de la grilla?\n",
    "    done = bool(self.agent_pos == 0)\n",
    "\n",
    "    # Asignamos recompensa sólo cuando el agente llega a su objetivo\n",
    "    # (recompensa = 0 en todos los demás estados)\n",
    "    reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "    # gym también nos permite devolver información adicional, ej. en atari: \n",
    "    # las vidas restantes del agente (no usaremos esto por ahora)\n",
    "    info = {}\n",
    "\n",
    "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
    "\n",
    "  def render(self, mode='console'):\n",
    "    if mode != 'console':\n",
    "      raise NotImplementedError()\n",
    "    # en nuestra interfaz de consola, representamos el agente como una cruz, y \n",
    "    # el resto como un punto\n",
    "    print(\".\" * self.agent_pos, end=\"\")\n",
    "    print(\"x\", end=\"\")\n",
    "    print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "  def close(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flB9-j1SoIJP",
    "outputId": "67ccb533-c91c-441a-8f85-72346c10e1ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 1042     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.6        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018975712 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.675      |\n",
      "|    explained_variance   | -0.69       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0226     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.0142      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 24.8       |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 694        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02359224 |\n",
      "|    clip_fraction        | 0.423      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.614     |\n",
      "|    explained_variance   | 0.382      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.048     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0518    |\n",
      "|    value_loss           | 0.0212     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.1        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 658         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042742252 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0947     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0485     |\n",
      "|    value_loss           | 0.00781     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 11         |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09064309 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.306     |\n",
      "|    explained_variance   | 0.234      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0429    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0391    |\n",
      "|    value_loss           | 0.00141    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.3        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 630         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005344364 |\n",
      "|    clip_fraction        | 0.0671      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.219      |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0311     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 0.000396    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.3         |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073009655 |\n",
      "|    clip_fraction        | 0.0655      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0753     |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0294     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    value_loss           | 0.000235    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 9.14          |\n",
      "|    ep_rew_mean          | 1             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 621           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00041007256 |\n",
      "|    clip_fraction        | 0.00933       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0341       |\n",
      "|    explained_variance   | 0.924         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00104       |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.00492      |\n",
      "|    value_loss           | 5.31e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 9.08          |\n",
      "|    ep_rew_mean          | 1             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 615           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 29            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030224366 |\n",
      "|    clip_fraction        | 0.00566       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.023        |\n",
      "|    explained_variance   | 0.955         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00891      |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.00384      |\n",
      "|    value_loss           | 2.81e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 9.06          |\n",
      "|    ep_rew_mean          | 1             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 608           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 33            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024326338 |\n",
      "|    clip_fraction        | 0.00322       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0191       |\n",
      "|    explained_variance   | 0.966         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000338      |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.00229      |\n",
      "|    value_loss           | 2.09e-05      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = GoLeftEnv(grid_size=10)\n",
    "env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1).learn(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1Cat8Gcr7sT"
   },
   "source": [
    "Ver agente entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "q5vsi3eSr7sT",
    "outputId": "d7e5df00-f408-46b8-c892-c438d211c720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........x..\n",
      ".......x...\n",
      "......x....\n",
      ".....x.....\n",
      "....x......\n",
      "...x.......\n",
      "..x........\n",
      ".x.........\n",
      ".........x.\n",
      "........x..\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "for i in range(10):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render(mode='console')\n",
    "    if dones[0]:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgrWKJlsoIJT"
   },
   "source": [
    "## Normalización de features y recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIfn3o99oIJT"
   },
   "source": [
    "Stable-Baselines3 tiene predefinidos un conjunto de **wrappers** genericos que pueden utilizarse para preprocesar las observaciones que llegan al agente RL, desacomplando del mismo el prepocesamiento.\n",
    "\n",
    "Entre las funcionalidades disponibles tenemos:\n",
    "* **VecFrameStack**: Se utiliza cuando la observación que percibe el agente es una imagen. Sirve para expandir el espacio de estados apilando N frames de manera conjunta.\n",
    "* **VecNormalize**: Se utiliza para normalizar las observaciones y/o recompenzas que percibe el agente a $\\mu=0$ y $\\sigma=1$. También permite cortar valores de observaciones y/o recompensas que excedan un rango establecido. \n",
    "* **VecCheckNan**: Se utiliza para trackear los estados del entorno que generan que los gradientes de la NN se hagan NaN.\n",
    "* **VecVideoRecorder**: Se utiliza para exportar el funcionamiento de la política aprendida por el agente a un video (MP4).\n",
    "\n",
    "Ademas, se pueden crear **wrappers** personalizados extendiendo la clase **VecEnvWrapper**:\n",
    "\n",
    "```\n",
    "class MiWrapper(VecEnvWrapper):\n",
    "    [...]\n",
    "```\n",
    "### Ejemplo (VecNormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3VOh1HQCVJNA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f2a68a81a50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env = DummyVecEnv([lambda: env])  # Multiple vectorize environments\n",
    "# Observations and reward normalization\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)\n",
    "\n",
    "model = DQN('MlpPolicy', env)\n",
    "model.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exel4Gr8oIJQ"
   },
   "source": [
    "# RL-baselines3-zoo\n",
    "\n",
    "Colección de agentes RL y herramientas útiles para ejecutarlos, evaluarlos e incluso hacer videos con ellos. Los agentes de este repo están preparados con la configuración requerida para los distintos tipos de entornos, incluyendo Atari, PyBullet y entornos clásicos, incluyendo configuraciones e híper-parámetros que producen buenas políticas para tales entornos.\n",
    "\n",
    "Esta librería ofrece un muy buen punto de partida para utilizar agentes / entornos personalizados, ya que ofrece una [interfaz](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/train.py) fácilmente adaptable a nuestras necesidades.\n",
    "\n",
    "Si se usan entornos personalizados con rl-baselines3-zoo, debe tenerse en cuenta que se deben definir todos los híper-parámetros de antemano sea al instanciar el agente o en la carpeta /rl-baselines3-zoo/hyperparams; de lo contrario arrojará error por no encontrar qué híper-parámetro usar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para empezar, clonamos el repo de rl-baselines3-zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'rl-baselines3-zoo' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6TCyu_UoIJR"
   },
   "source": [
    "## Ejecución\n",
    "\n",
    "Los agentes pueden ser llamados desde la consola mediante comandos como\n",
    "\n",
    "`python train.py --algo algo_name --env env_id`\n",
    "\n",
    "Los cuales pueden ser llamados usando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jBLMVxhIoIJR"
   },
   "outputs": [],
   "source": [
    "os.chdir('rl-baselines3-zoo/')\n",
    "\n",
    "args = [\n",
    "    '-n', str(100000),\n",
    "    '--algo', 'ppo',\n",
    "    '--env', 'CartPole-v1'\n",
    "]\n",
    "\n",
    "p = Popen(['python', 'train.py'] + args,\n",
    "                               stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "output, err = p.communicate()\n",
    "rc = p.returncode\n",
    "os.chdir(cwd)\n",
    "assert rc == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40H4hNcqoIJR"
   },
   "source": [
    "Ver en acción el agente entrenado (nota: no disponible en Google Colab, requiere ffmpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cNija96loIJR"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    os.chdir('rl-baselines3-zoo/')\n",
    "\n",
    "    args = [\n",
    "        '--algo', 'ppo',\n",
    "        '--env', 'CartPole-v1',\n",
    "        '--folder', 'logs/'\n",
    "    ]\n",
    "\n",
    "    p = Popen(['python', 'enjoy.py'] + args,\n",
    "                                   stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "    output, err = p.communicate()\n",
    "    rc = p.returncode\n",
    "    os.chdir(cwd)\n",
    "    assert rc == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSfIitK7oIJR"
   },
   "source": [
    "También es posible grabar un video! Ver https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#record-a-video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Phq975BYoIJR"
   },
   "source": [
    "Ver curva de aprendizaje obtenida por el agente desde *funciones_utiles.plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Q8EA41oQoIJS"
   },
   "outputs": [],
   "source": [
    "os.chdir('rl-baselines3-zoo/')\n",
    "\n",
    "args = [\n",
    "    '--algo', 'ppo',\n",
    "    '--env', 'CartPole-v1',\n",
    "    '--exp-folder', 'logs/'\n",
    "]\n",
    "\n",
    "p = Popen(['python', '-m', 'scripts.plot_train'] + args, stdout=PIPE)\n",
    "output, err = p.communicate()\n",
    "rc = p.returncode\n",
    "os.chdir(cwd)\n",
    "\n",
    "assert rc == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__1mgFR0oIJT",
    "outputId": "2c355f37-24ae-4e78-ea4c-ac66d3003c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Figure(640x480)\\n'\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWzAHj6qoIJT"
   },
   "source": [
    "## Híper-parámetros\n",
    "\n",
    "rl-baselines-zoo provee híper-parámetros que resultan en curvas de aprendizaje que convergen en buena cantidad de entornos. Estos híper-parámetros pueden verse en cada uno de los archivos YAML de cada algoritmo, [acá](https://github.com/DLR-RM/rl-baselines3-zoo/tree/master/hyperparams).\n",
    "\n",
    "También provee funcionalidad para optimizar los híper-parámetros con la librería [Optuna]( https://github.com/optuna/optuna). En los mismos se incluyen rangos de híper-parámetros que se usaron para optimizar entornos como los de PyBullet, y son fácilmente modificables para adaptarlo a nuestros propios entornos. Para ver cómo se llama a la interfaz de Optuna ver [este código](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py).\n",
    "\n",
    "Nota: **Optuna come muchos recursos!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPZvkgTKr7sW"
   },
   "source": [
    "# Entrenando otros agentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhnOJ2CXr7sW"
   },
   "source": [
    "### Algunos conjuntos de entornos\n",
    "\n",
    "CartPole es una excelente línea base (de hecho suele ser la prueba preliminar de todo nuevo algoritmo), porque tiene recompensas constantes pero requiere cierta solidez por parte del algoritmo para hacerlo converger al óptimo.\n",
    "\n",
    "No obstante, al implementar un algoritmo, es deseable que el mismo pueda desenvolverse de forma consistente en varios grupos de entornos. A continuación va una lista con varios entornos que sirven como prueba:\n",
    "\n",
    "| Entornos                                                                                                           | Estados            | Acciones            | Dificultad      | Implementado por                                                                                                                                                            |\n",
    "|--------------------------------------------------------------------------------------------------------------------|--------------------|---------------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Clásicos (CartPole, MountainCar, Pendulum, Deep sea (testea exploración), umbrella (testea asignación de crédito)) | Discretos/Continuos          | Continuas/discretas | Baja/Media      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments) y [BSuite](https://github.com/deepmind/bsuite)                                                                                                                          |\n",
    "| Grilla (desde pequeñas donde hay que salir hasta grandes con muchas habitaciones y subproblemas)                   | Discretos/imágenes | Discretas           | Baja/Media/Alta | [gym-minigrid](https://github.com/maximecb/gym-minigrid)                                                                                                                    |\n",
    "| Grilla/plataforma/estilo Atari, generados proceduralmente                                                          | Imágenes           | Discretas           | Media/Alta      | [Gym](https://github.com/openai/procgen)                                                                                                                                    |\n",
    "| Plataforma 2D, como LunarLander o BipedalWalker                                                                    | Continuos          | Continuas/discretas | Media/Alta      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments)                                                                                                                                                                         |\n",
    "| Primera persona en 3D                                                                                              | Imágenes           | Continuas/discretas | Media/Alta      | [Deepmind](https://github.com/deepmind/lab)                                                                                                                                 |\n",
    "| Simulación física de pequeños robots                                                                               | Continuos          | Continuas           | Media/Alta      | Gym (con el motor MuJoCo o su versión open-source, [PyBullet](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.wz5to0x8kqmr)) |\n",
    "| Atari                                                                                                              | Continuos          | Discretas           | Media/Alta      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments) (son todos aquellos entornos que terminan en \"*-v4*\"                                                                                                                                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5m0SMewr7sW"
   },
   "source": [
    "## Resumen de algunos algoritmos\n",
    "\n",
    "Se resumen ahora varios algoritmos del estado del arte de aprendizaje por refuerzos profundo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Vd0J1zxr7sW"
   },
   "source": [
    "| Algoritmo | Tipo       | Espacio de acciones | Resumen rápido                                                                                                                                                | Artículo                         |\n",
    "|-----------|------------|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|\n",
    "| DQN       | Off-policy | Discretas           | Extiende Q-Learning a deep learning. En Stable-baselines, DQN incluye todas las mejoras ya incorporadas.                                                      | https://arxiv.org/abs/1312.5602  |\n",
    "| ACER      | Off-policy | Discretas           | Combina una arquitectura actor-critic con un buffer y repetición de experiencia.                                                                              | https://arxiv.org/abs/1611.01224 |\n",
    "| A3C       | On-policy  | Ambos               | Múltiples agentes corriendo en múltiples instancias del ambiente, acumulando sus gradientes y actualizándolos tras un cierto tiempo.                          | https://arxiv.org/abs/1602.01783 |\n",
    "| PPO       | On-policy  | Ambos               | Punta de lanza de policy gradient, incluye mecanismo para que el gradiente actualice de forma acotada, mejorando drásticamente la estabilidad.             | https://arxiv.org/abs/1707.06347 |\n",
    "| DDPG      | Off-policy | Continuas           | Como en espacios de acciones continuos es muy difícil encontrar $\\max_a Q(s,a)$, se aproxima via $Q(s, a(s \\mid \\theta_a))$, siendo $a$ un actor estocástico. | https://arxiv.org/abs/1509.02971 |\n",
    "| TD3       | Off-policy | Continuas           | Mejora DDPG utilizando dos funciones $Q$ y retrasando la actualización para reducir la sobreestimación de $Q$.                                                | https://arxiv.org/abs/1802.09477 |\n",
    "| SAC       | Off-policy | Continuas           | Usa dos funciones $Q$, introduce el bonus por entropía y usa un actor estocástico que muestrea acciones según una política $\\pi$.                             | https://arxiv.org/abs/1801.01290 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9XDV5lmr7sW"
   },
   "source": [
    "## Resumen de algunas herramientas/trucos comúnmente usados\n",
    "\n",
    "* Experience replay/buffer de experiencia: guarda las experiencias en un buffer para poder usarlas repetidamente durante el entrenamiento.\n",
    "    * Ventajas: experiencias raras pero muy relevantes (ej: que tienen mucho error de actualización) quedan guardadas en memoria, pudiendo ser usadas repetidamente para aprender sin necesidad de esperar a que se repitan.\n",
    "    \n",
    "    En métodos de gradiente de política, en cambio, el aprendizaje queda reflejado en los pesos, lo cuál puede hacer que un agente no se desenvuelva correctamente en entornos de recompensa escasa como MountainCar.\n",
    "    \n",
    "    * Desventajas: requiere considerable RAM, son usables solamente en algoritmos off-policy y su muestreo no necesariamente refleja la probabilidad real de tener esas experiencias en el entorno (añadiendo sesgo), por lo que es recomendable usarlo junto con importance sampling.\n",
    "    \n",
    "* Importance sampling: aplica un descuento a las actualizaciones a partir de experience replay relacionado a cuán probable era realizar esa transición.\n",
    "* Entropía: añade un bonus a la función de recompensa para que bonifique políticas $\\pi(s \\mid a)$ que tengan mayor entropía que otras, motivando la exploración del agente.\n",
    "* Juntar varias secuencias de imágenes. Usado principalmente en entornos de Atari para poder evaluar la dirección de movimientos.\n",
    "* Normalización de recompensas/estados. Normaliza las recompensas y observaciones usualmente con una media móvil, de modo tal que las observaciones/recompensas reflejen su relación y varianza con respecto a las demás.\n",
    "* Clipping (recorte) de recompensas. Se usaba principalmente en entornos de Atari con algoritmos tipo DQN para recortar el impacto que las distintas recompensas tenían, a una constante (ej: 1). Suele usarse como una cota máxima de recompensas/observaciones normalizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iJQRnPMr7sX"
   },
   "source": [
    "# Actividades\n",
    "\n",
    "1. Crear tu propio entorno y entrenar agentes RL en el mismo. Analizar la convergencia con distintos algoritmos* (ej: PPO, DQN), resultados con distintas funciones de recompensa e híper-parámetros. \n",
    "\n",
    "    Algunas ideas:\n",
    "\n",
    "    * Transformar GoLeftEnv en una grilla 2D, añadir paredes / trampas / agua.\n",
    "    * Crear un entorno que juegue a algún juego como el ta-te-ti.\n",
    "    * Crea un entorno totalmente nuevo que sea de tu interés!\n",
    "\n",
    "2. Entrena agentes en entornos más complejos con stable-baselines/rl-baselines-zoo. Tener en cuenta:\n",
    "\n",
    "    * Google Colab tiene una limitante en cuanto a cantidad de recursos de CPU/GPU (incluido un \"rendimiento decreciente silencioso\"), lo cuál reduce la capacidad de entrenar distintos entornos.\n",
    "    * Si el entorno no está implementado en stable-baselines, debe hacerse un wrapper a mano, lo que puede ser sencillo o puede llevar algo más de trabajo, teniendo que tocar código subyacente de la librería. \n",
    "\n",
    "\\* pueden ser usando stable-baselines/rl-baselines-zoo o bien utilizando algún otro algoritmo (incluso tabular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5YbRAbyoIJT"
   },
   "source": [
    "# Recursos adicionales\n",
    "\n",
    "* [Discord de RL](https://discord.gg/xhfNqQv) (muy activo y recomendado) y su [wiki de recursos](https://github.com/andyljones/reinforcement-learning-discord-wiki/wiki#rl-frameworks--reference-implementations)\n",
    "* [Excelente recurso para aprender más de deep RL](https://spinningup.openai.com/en/latest/spinningup/spinningup.html)\n",
    "* [Framework adicional de aprendizaje por refuerzos a gran escala](https://docs.ray.io/en/master/rllib.html)\n",
    "* [Awesome Deep RL](https://github.com/kengz/awesome-deep-rl)\n",
    "* [Awesome RL envs](https://github.com/clvrai/awesome-rl-envs)\n",
    "* [Comunidad de Bots de RL para Rocket League](https://rlbot.org/)\n",
    "* [Blog de Lilian Weng de Deep RL, robótica y NLP](https://lilianweng.github.io/lil-log/)\n",
    "* [The Nuts and Bolts of Deep RL Research](http://joschu.net/docs/nuts-and-bolts.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwXMoJy9oIJT"
   },
   "source": [
    "FIN"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SD-FSSuKoIJF",
    "uUUkScRxoIJQ"
   ],
   "include_colab_link": true,
   "name": "stable_baselines.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
